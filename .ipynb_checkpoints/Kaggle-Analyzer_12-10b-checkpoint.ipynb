{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get imports and initialize matplotlib..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get metadata from csv file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = 16 # num of perspectives\n",
    "zs = 17 # num of outputs zones\n",
    "\n",
    "ids = []\n",
    "targets = OrderedDict()\n",
    "with open('labels.csv') as f:\n",
    "    header = f.readline()\n",
    "    for line in f:\n",
    "        idzone, label = line.split(',')\n",
    "        sampleid, zone = idzone.split('_')\n",
    "        zone_number = int(zone[len('Zone'):]) # string looks like \"Zone8\" so this chops off \"Zone\"\n",
    "        zone_number -= 1\n",
    "        \n",
    "        if sampleid not in targets:\n",
    "            ids.append(sampleid)\n",
    "            targets[sampleid] = np.zeros(zs)\n",
    "        targets[sampleid][zone_number] = float(label)\n",
    "\n",
    "random.shuffle(ids) # add randomness to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our (giant...) matrices for training. Takes up roughly 12 GB of system memory (thankfully mine as 32)\n",
    "\n",
    "Set n to a value to reduce this size for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 8192, 660)\n",
      "(50, 17)\n"
     ]
    }
   ],
   "source": [
    "X_train = [] # training data\n",
    "y_train = [] # training one hot labels\n",
    "l_train = [] # training labels\n",
    "\n",
    "w = 512 * ps\n",
    "h = 660\n",
    "\n",
    "\n",
    "i = 0\n",
    "n = 50#len(ids)\n",
    "\n",
    "for sid in ids:\n",
    "    path = 'data/{}.aps.npz'.format(sid)\n",
    "\n",
    "    npzfile = np.load(path)\n",
    "    data = npzfile['arr_0']\n",
    "    \n",
    "    X_train.append(data)\n",
    "    l_train.append(np.argmax(targets[sid]))\n",
    "    if (np.argmax(targets[sid]) == 0):\n",
    "        targets[sid][0] = 1\n",
    "    y_train.append(targets[sid])\n",
    "    \n",
    "    i += 1\n",
    "    print (i, \"of\", n, end=\"\\r\")\n",
    "    if i >= n:\n",
    "        break\n",
    "\n",
    "X_train = np.moveaxis(X_train, 3, 1)\n",
    "y_train = np.moveaxis(y_train, 0, 0) # to convert into a matrix (vs a dictionary?)\n",
    "\n",
    "X_train = np.reshape(X_train, [n, w, h])\n",
    "X_train = X_train.astype(np.float32)\n",
    "print (np.shape(X_train))\n",
    "print (np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.] 1\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.] 2\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.] 4\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 3\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 5\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.] 2\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.] 9\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.] 4\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.] 4\n",
      "[ 0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.] 1\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.] 2\n",
      "[ 0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.] 3\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.] 14\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.] 4\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.] 12\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.] 13\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.] 1\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 7\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.] 7\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.] 6\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.] 14\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.] 4\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.] 10\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.] 10\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.] 2\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.] 7\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.] 6\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 7\n",
      "[ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 1\n",
      "[ 0.  1.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 1\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 0\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 7\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.] 6\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.] 9\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 5\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] 5\n"
     ]
    }
   ],
   "source": [
    "for x in range(max(n,50)):\n",
    "    print (y_train[x], l_train[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the perspective is the last axis in the matrix, move it to be the second matrix so it's more logical (doesn't actually make a difference from a training perspective)\n",
    "\n",
    "X_train should now be in the form [sampleIndex, perspective, x, y]\n",
    "\n",
    "Also, moveaxis somehow does this operation while using zero extra memory, no idea how but clearly whoever made it had a better professor for algorithms than Gelfond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print (X_train[0]) # first sample\n",
    "print (y_train[0])\n",
    "print (l_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.pyplot import imshow \n",
    "# figsize(15,4)\n",
    "# im = imshow(np.flipud(x_test[0,:,:]).T, cmap = 'viridis') # 1st sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make .train() ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model hyper parameters\n",
    "learning_rate = 0.0005\n",
    "\n",
    "def conv_layer(input_data, num_input_channels, num_filters, filter_shape, name):\n",
    "    # setup the filter input shape for tf.nn.conv_2d\n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels,\n",
    "                      num_filters]\n",
    "\n",
    "    # initialise weights and bias for the filter\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),\n",
    "                                      name=name+'_W')\n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "\n",
    "    # setup the convolutional layer operation\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    # add the bias\n",
    "    out_layer += bias\n",
    "\n",
    "    # apply a ReLU non-linear activation\n",
    "    out_layer = tf.nn.relu(out_layer)\n",
    "    return out_layer\n",
    "    \n",
    "def pool(out_layer, pool_shape):\n",
    "    # now perform max pooling\n",
    "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "    strides = [1, 3, 2, 1]\n",
    "    out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides, \n",
    "                               padding='VALID')\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, w, h], name=\"inputx\")\n",
    "\n",
    "x_shaped = tf.reshape(x, [-1, w, h, 1])\n",
    "# x_shaped = tf.cast(x_shaped, tf.float32)\n",
    "\n",
    "y = tf.placeholder(tf.float16, [None, zs])\n",
    "\n",
    "l1 = conv_layer(x_shaped, 1, 3, [3, 3], name='l1')\n",
    "l2 = conv_layer(l1, 3, 16, [3, 3], name='l2')\n",
    "l3 = pool(l2, [2,2])\n",
    "l4 = conv_layer(l3, 16, 16, [3, 3], name='l4')\n",
    "l5 = conv_layer(l4, 16, 16, [3, 3], name='l5')\n",
    "l6 = pool(l5, [2,2])\n",
    "l7 = conv_layer(l6, 16, 32, [3, 3], name='l7')\n",
    "l8 = conv_layer(l7, 32, 32, [3, 3], name='l8')\n",
    "l9 = conv_layer(l8, 32, 32, [3, 3], name='l9')\n",
    "l10 = pool(l9, [2,2])\n",
    "l11 = conv_layer(l10, 32, 32, [3, 3], name='l11')\n",
    "l12 = conv_layer(l11, 32, 32, [3, 3], name='l12')\n",
    "l13 = conv_layer(l12, 32, 32, [3, 3], name='l13')\n",
    "l14 = pool(l13, [2,2])\n",
    "l15 = conv_layer(l14, 32, 32, [3, 3], name='l15')\n",
    "l16 = conv_layer(l15, 32, 32, [3, 3], name='l16')\n",
    "l17 = conv_layer(l16, 32, 32, [3, 3], name='l17')\n",
    "l18 = pool(l17, [2,2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#dilation, seperable filter\n",
    "\n",
    "dumb = 21760 # use shape_check to get this\n",
    "\n",
    "flattened = tf.reshape(l18, [-1, dumb])\n",
    "\n",
    "wd1 = tf.Variable(tf.truncated_normal([dumb, 1024], stddev=0.1), name='wd1')\n",
    "bd1 = tf.Variable(tf.truncated_normal([1024], stddev=0.01), name='bd1')\n",
    "dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
    "dense_layer1 = tf.nn.relu(dense_layer1)\n",
    "\n",
    "wd2 = tf.Variable(tf.truncated_normal([1024, 1024], stddev=0.1), name='wd2')\n",
    "bd2 = tf.Variable(tf.truncated_normal([1024], stddev=0.01), name='bd2')\n",
    "dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
    "dense_layer2 = tf.nn.relu(dense_layer2)\n",
    "\n",
    "wd3 = tf.Variable(tf.truncated_normal([1024, zs], stddev=0.1), name='wd3')\n",
    "bd3 = tf.Variable(tf.truncated_normal([zs], stddev=0.01), name='bd3')\n",
    "dense_layer3 = tf.matmul(dense_layer2, wd3) + bd3\n",
    "dense_layer3 = tf.nn.relu(dense_layer3)\n",
    "\n",
    "y_ = tf.nn.softmax(dense_layer3)\n",
    "\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer3, labels=y)\n",
    "\n",
    "# z_weights = tf.constant([10000, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], tf.float32)\n",
    "\n",
    "# weighted_cross_entropy = tf.multiply(cross_entropy, z_weights)\n",
    "\n",
    "weighted_cross_entropy = tf.exp(tf.div(tf.constant(1.), tf.subtract(tf.constant(1.), cross_entropy)))\n",
    "\n",
    "cost = tf.reduce_mean(weighted_cross_entropy)\n",
    "\n",
    "\n",
    "# add an optimiser\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float16))\n",
    "\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     # initialise the variables\n",
    "#     sess.run(init_op)\n",
    "#     total_batch = int(len(y_train) / batch_size)\n",
    "#     for epoch in range(epochs):\n",
    "#         avg_cost = 0\n",
    "#         for i in range(total_batch):\n",
    "#             batch_x, batch_y = X_train[0][(total_batch*epoch+i):min(len(y_train),(total_batch*(epoch+1)))]\n",
    "#             _, c = sess.run([optimiser, cross_entropy], \n",
    "#                             feed_dict={x: batch_x, y: batch_y})\n",
    "#             avg_cost += c / total_batch\n",
    "#         test_acc = sess.run(accuracy, \n",
    "#                        feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "#         print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost), \" \n",
    "#                  test accuracy: {:.3f}\".format(test_acc))\n",
    "\n",
    "#     print(\"\\nTraining complete!\")\n",
    "#     print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "shape_check = tf.shape(l18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     # initialise the variables\n",
    "#     init_op = tf.global_variables_initializer()\n",
    "#     sess.run(init_op)\n",
    "    \n",
    "#     print(sess.run(shape_check, feed_dict={x: X_train[0:1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run .train() ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:\\ML\\chkpts\\VGG_12-08-test\\checkpoint_2.ckpt\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [1024,17] rhs shape= [1024,18]\n\t [[Node: save/Assign_97 = Assign[T=DT_FLOAT, _class=[\"loc:@wd3\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](wd3/Adam_1, save/RestoreV2_97/_5)]]\n\nCaused by op 'save/Assign_97', defined at:\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-cc145a6a78ac>\", line 9, in <module>\n    saver = tf.train.Saver()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1140, in __init__\n    self.build()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 688, in build\n    restore_sequentially, reshape)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 419, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 274, in assign\n    validate_shape=validate_shape)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 43, in assign\n    use_locking=use_locking, name=name)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1024,17] rhs shape= [1024,18]\n\t [[Node: save/Assign_97 = Assign[T=DT_FLOAT, _class=[\"loc:@wd3\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](wd3/Adam_1, save/RestoreV2_97/_5)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32md:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\misc\\anaconda\\envs\\tf2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [1024,17] rhs shape= [1024,18]\n\t [[Node: save/Assign_97 = Assign[T=DT_FLOAT, _class=[\"loc:@wd3\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](wd3/Adam_1, save/RestoreV2_97/_5)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-cc145a6a78ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\\\checkpoint_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcheckpoint_num\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".ckpt.meta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\\\checkpoint_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcheckpoint_num\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid model checkpoint to validate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1558\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1560\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1562\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [1024,17] rhs shape= [1024,18]\n\t [[Node: save/Assign_97 = Assign[T=DT_FLOAT, _class=[\"loc:@wd3\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](wd3/Adam_1, save/RestoreV2_97/_5)]]\n\nCaused by op 'save/Assign_97', defined at:\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-cc145a6a78ac>\", line 9, in <module>\n    saver = tf.train.Saver()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1140, in __init__\n    self.build()\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 688, in build\n    restore_sequentially, reshape)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 419, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 274, in assign\n    validate_shape=validate_shape)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 43, in assign\n    use_locking=use_locking, name=name)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1024,17] rhs shape= [1024,18]\n\t [[Node: save/Assign_97 = Assign[T=DT_FLOAT, _class=[\"loc:@wd3\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](wd3/Adam_1, save/RestoreV2_97/_5)]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 3       # num samples to validate per batch (this seems to be the max the card can handle at at time?)\n",
    "\n",
    "\n",
    "model_name = \"VGG_12-08-test\"\n",
    "checkpoint_num = \"2\" # **** If not valid, will wipe old model!!! ****\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# setup the initialisation operator\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "path = \"D:\\\\ML\\\\chkpts\\\\\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if os.path.exists(path+model_name) and os.path.exists(path+model_name+\"\\\\checkpoint_\"+checkpoint_num+\".ckpt.meta\"):\n",
    "        saver.restore(sess, path+model_name+\"\\\\checkpoint_\"+checkpoint_num+\".ckpt\")\n",
    "    else:\n",
    "        print(\"Invalid model checkpoint to validate\")\n",
    "        sys.exit()\n",
    "    \n",
    "    n = len(y_train)\n",
    "    batches = math.ceil(n / batch_size)\n",
    "    \n",
    "#     correct_preds = 0\n",
    "    for i in range(batches):\n",
    "        s = i * batch_size # start\n",
    "        e = min((i+1) * batch_size, n)\n",
    "        x_to_run = X_train[s:e]\n",
    "        y_to_run = y_train[s:e]\n",
    "        preds = sess.run(y_, feed_dict={x: x_to_run, y: y_to_run})\n",
    "#         correct_preds += int(round(test_acc * (e - s)))\n",
    "        print(\"Ran Batch \",i,\" out of \",batches,\"\\nResults: \",preds, \"\\nLabels: \",y_train[s:e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# configconfig = tf.ConfigProto()\n",
    "# configconfig.log_device_placement = True\n",
    "# configconfig.gpu_options.allow_growth = True\n",
    "\n",
    "# config = tf.contrib.learn.RunConfig(session_config = configconfig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
