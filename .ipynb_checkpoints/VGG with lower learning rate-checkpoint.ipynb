{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get imports and initialize matplotlib..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get metadata from csv file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = 16 # num of perspectives\n",
    "zs = 17 # num of outputs zones\n",
    "\n",
    "ids = []\n",
    "targets = OrderedDict()\n",
    "with open('labels.csv') as f:\n",
    "    header = f.readline()\n",
    "    for line in f:\n",
    "        idzone, label = line.split(',')\n",
    "        sampleid, zone = idzone.split('_')\n",
    "        zone_number = int(zone[len('Zone'):]) # string looks like \"Zone8\" so this chops off \"Zone\"\n",
    "        zone_number -= 1\n",
    "        \n",
    "        if sampleid not in targets:\n",
    "            ids.append(sampleid)\n",
    "            targets[sampleid] = np.zeros(zs)\n",
    "        targets[sampleid][zone_number] = float(label)\n",
    "\n",
    "random.shuffle(ids) # add randomness to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our (giant...) matrices for training. Takes up roughly 12 GB of system memory (thankfully mine as 32)\n",
    "\n",
    "Set n to a value to reduce this size for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:26: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1147 of 1147          of 1147         \r"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0bc38eb85804>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    230\u001b[0m            [5, 6]])\n\u001b[0;32m    231\u001b[0m     \"\"\"\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\misc\\anaconda\\envs\\tf2\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train = [] # training data\n",
    "y_train = [] # training one hot labels\n",
    "l_train = [] # training labels\n",
    "X_test  = []\n",
    "y_test  = []\n",
    "\n",
    "w = 512 * ps\n",
    "h = 660\n",
    "\n",
    "\n",
    "i = 0\n",
    "n = 50#len(ids)\n",
    "\n",
    "for sid in ids:\n",
    "    path = 'data/{}.aps.npz'.format(sid)\n",
    "\n",
    "    npzfile = np.load(path)\n",
    "    data = npzfile['arr_0']\n",
    "    \n",
    "    if sid not in rand_indexes:\n",
    "        X_train.append(data)\n",
    "        y_train.append(targets[sid])\n",
    "        l_train.append(np.argmax(targets[sid]))\n",
    "    else:\n",
    "        X_test.append(data)\n",
    "        y_test.append(targets[sid])\n",
    "    \n",
    "    i += 1\n",
    "    print (i, \"of\", n, end=\"\\r\")\n",
    "    if i >= n:\n",
    "        break\n",
    "        \n",
    "X_train = np.moveaxis(X_train, 3, 1)\n",
    "y_train = np.moveaxis(y_train, 0, 0) # to convert into a matrix (vs a dictionary?)\n",
    "\n",
    "X_train = np.reshape(X_train, [n, w, h])\n",
    "X_train = X_train.astype(np.float32)\n",
    "print (np.shape(X_train))\n",
    "print (np.shape(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the perspective is the last axis in the matrix, move it to be the second matrix so it's more logical (doesn't actually make a difference from a training perspective)\n",
    "\n",
    "X_train should now be in the form [sampleIndex, perspective, x, y]\n",
    "\n",
    "Also, moveaxis somehow does this operation while using zero extra memory, no idea how but clearly whoever made it had a better professor for algorithms than Gelfond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train[0]) # first sample\n",
    "print (y_train[0])\n",
    "print (l_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow \n",
    "figsize(15,4)\n",
    "im = imshow(np.flipud(X_train[0,:,:]).T, cmap = 'viridis') # 1st sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make .train() ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model hyper parameters\n",
    "learning_rate = 0.00005\n",
    "\n",
    "def conv_layer(input_data, num_input_channels, num_filters, filter_shape, name):\n",
    "    # setup the filter input shape for tf.nn.conv_2d\n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels,\n",
    "                      num_filters]\n",
    "\n",
    "    # initialise weights and bias for the filter\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),\n",
    "                                      name=name+'_W')\n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "\n",
    "    # setup the convolutional layer operation\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    # add the bias\n",
    "    out_layer += bias\n",
    "\n",
    "    # apply a ReLU non-linear activation\n",
    "    out_layer = tf.nn.relu(out_layer)\n",
    "    return out_layer\n",
    "    \n",
    "def pool(out_layer, pool_shape):\n",
    "    # now perform max pooling\n",
    "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "    strides = [1, 3, 2, 1]\n",
    "    out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides, \n",
    "                               padding='VALID')\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, w, h], name=\"inputx\")\n",
    "\n",
    "x_shaped = tf.reshape(x, [-1, w, h, 1])\n",
    "# x_shaped = tf.cast(x_shaped, tf.float32)\n",
    "\n",
    "y = tf.placeholder(tf.float16, [None, zs])\n",
    "\n",
    "l1 = conv_layer(x_shaped, 1, 3, [3, 3], name='l1')\n",
    "l2 = conv_layer(l1, 3, 16, [3, 3], name='l2')\n",
    "l3 = pool(l2, [2,2])\n",
    "l4 = conv_layer(l3, 16, 16, [3, 3], name='l4')\n",
    "l5 = conv_layer(l4, 16, 16, [3, 3], name='l5')\n",
    "l6 = pool(l5, [2,2])\n",
    "l7 = conv_layer(l6, 16, 32, [3, 3], name='l7')\n",
    "l8 = conv_layer(l7, 32, 32, [3, 3], name='l8')\n",
    "l9 = conv_layer(l8, 32, 32, [3, 3], name='l9')\n",
    "l10 = pool(l9, [2,2])\n",
    "l11 = conv_layer(l10, 32, 32, [3, 3], name='l11')\n",
    "l12 = conv_layer(l11, 32, 32, [3, 3], name='l12')\n",
    "l13 = conv_layer(l12, 32, 32, [3, 3], name='l13')\n",
    "l14 = pool(l13, [2,2])\n",
    "l15 = conv_layer(l14, 32, 32, [3, 3], name='l15')\n",
    "l16 = conv_layer(l15, 32, 32, [3, 3], name='l16')\n",
    "l17 = conv_layer(l16, 32, 32, [3, 3], name='l17')\n",
    "l18 = pool(l17, [2,2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#dilation, seperable filter\n",
    "\n",
    "dumb = 21760 # use shape_check to get this\n",
    "\n",
    "flattened = tf.reshape(l18, [-1, dumb])\n",
    "\n",
    "wd1 = tf.Variable(tf.truncated_normal([dumb, 1024], stddev=0.1), name='wd1')\n",
    "bd1 = tf.Variable(tf.truncated_normal([1024], stddev=0.01), name='bd1')\n",
    "dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
    "dense_layer1 = tf.nn.relu(dense_layer1)\n",
    "\n",
    "wd2 = tf.Variable(tf.truncated_normal([1024, 1024], stddev=0.1), name='wd2')\n",
    "bd2 = tf.Variable(tf.truncated_normal([1024], stddev=0.01), name='bd2')\n",
    "dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
    "dense_layer2 = tf.nn.relu(dense_layer2)\n",
    "\n",
    "wd3 = tf.Variable(tf.truncated_normal([1024, zs], stddev=0.1), name='wd3')\n",
    "bd3 = tf.Variable(tf.truncated_normal([zs], stddev=0.01), name='bd3')\n",
    "dense_layer3 = tf.matmul(dense_layer2, wd3) + bd3\n",
    "dense_layer3 = tf.nn.relu(dense_layer3)\n",
    "\n",
    "y_ = tf.nn.softmax(dense_layer3)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer3, labels=y))\n",
    "\n",
    "\n",
    "# add an optimiser\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float16))\n",
    "\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     # initialise the variables\n",
    "#     sess.run(init_op)\n",
    "#     total_batch = int(len(y_train) / batch_size)\n",
    "#     for epoch in range(epochs):\n",
    "#         avg_cost = 0\n",
    "#         for i in range(total_batch):\n",
    "#             batch_x, batch_y = X_train[0][(total_batch*epoch+i):min(len(y_train),(total_batch*(epoch+1)))]\n",
    "#             _, c = sess.run([optimiser, cross_entropy], \n",
    "#                             feed_dict={x: batch_x, y: batch_y})\n",
    "#             avg_cost += c / total_batch\n",
    "#         test_acc = sess.run(accuracy, \n",
    "#                        feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "#         print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost), \" \n",
    "#                  test accuracy: {:.3f}\".format(test_acc))\n",
    "\n",
    "#     print(\"\\nTraining complete!\")\n",
    "#     print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "shape_check = tf.shape(l18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     # initialise the variables\n",
    "#     init_op = tf.global_variables_initializer()\n",
    "#     sess.run(init_op)\n",
    "    \n",
    "#     print(sess.run(shape_check, feed_dict={x: X_train[0:1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run .train() ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Hyper Params\n",
    "epochs           = 100000 # num of cycles to run batches for, checkpoint after each\n",
    "save_every       = 1      # num cycles between saves\n",
    "steps_per_batch  = 20     # num of steps to run per batch (low because we don't want too much learning on any one set of images at a time)\n",
    "batch_size       = 3      # num samples to train per batch (this seems to be the max the card can handle at at time?)\n",
    "test_percent = .10\n",
    "\n",
    "model_name = \"VGG_very_slow_learn_rate\" # model to save into\n",
    "\n",
    "restore_from = \"\"\n",
    "checkpoint_num = \"10000\" # **** If not valid, will wipe old model!!!\n",
    "\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# setup the initialisation operator\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "test_size = int(n*test_percent)\n",
    "n -= test_size\n",
    "\n",
    "path = \"D:\\\\ML\\\\chkpts\\\\\"\n",
    "\n",
    "# config = tf.ConfigProto(\n",
    "#         device_count = {'CPU': 0}\n",
    "#     )\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if os.path.exists(path+restore_from) and os.path.exists(path+restore_from+\"\\\\checkpoint_\"+checkpoint_num+\".ckpt.meta\"):\n",
    "        saver.restore(sess, path+restore_from+\"\\\\checkpoint_\"+checkpoint_num+\".ckpt\")\n",
    "        print(\"restored old model: \",restore_from)\n",
    "    else:\n",
    "        if not os.path.exists(path+model_name):\n",
    "            os.makedirs(path+model_name)\n",
    "        print(\"old model not found, starting a new model: \",model_name)\n",
    "        sess.run(init_op)\n",
    "    \n",
    "    \n",
    "    batches = math.ceil((n) / batch_size)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print (\"running epoch\",epoch+1,\"of\",epochs)\n",
    "        avg_cost = 0\n",
    "        for i in range(batches):\n",
    "            s = i * batch_size # start\n",
    "            e = min((i+1) * batch_size, n)\n",
    "            x_to_run = X_train[s:e]\n",
    "            y_to_run = y_train[s:e]\n",
    "            for j in range(steps_per_batch):\n",
    "                _, c = sess.run([optimiser, cross_entropy], \n",
    "                                 feed_dict={x: x_to_run, y: y_to_run})\n",
    "                avg_cost += c / steps_per_batch*batches\n",
    "            print(\"ran batch \",i+1,\"of\",batches, end='\\r')\n",
    "        print(\"\\rEnd Epoch\",(epoch+1),\" \\ncost =\", \"{:.3f}\".format(avg_cost))\n",
    "        \n",
    "        test_batches = math.ceil(z / batch_size)\n",
    "    \n",
    "        correct_preds = 0\n",
    "        for i in range(test_batches):\n",
    "            s = i * batch_size + n-test_size # start\n",
    "            e = min((i+1) * batch_size, test_size) + n-test_size\n",
    "            x_to_run = X_train[s:e]\n",
    "            y_to_run = y_train[s:e]\n",
    "            test_acc = sess.run(accuracy, feed_dict={x: x_to_run, y: y_to_run})\n",
    "            correct_preds += int(round(test_acc * (e - s)))\n",
    "            print(\"Ran Test Batch \",i,\" out of \",test_batches,\" got \",correct_preds,\" so far\", end=\"\\r\")\n",
    "        \n",
    "        print(\"Overall Test Accuracy after last epoch: \",correct_preds,\" out of \",test_size)\n",
    "\n",
    "        if epoch % save_every == 0:\n",
    "            checkpt = int(epoch/save_every)\n",
    "            print(\"Saving Checkpoint:\",checkpt)\n",
    "            saver.save(sess, path+model_name+\"\\\\checkpoint_\"+str(checkpt)+\".ckpt\")\n",
    "        print(\"=================================\");\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(sess.run(accuracy, feed_dict={x: X_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# configconfig = tf.ConfigProto()\n",
    "# configconfig.log_device_placement = True\n",
    "# configconfig.gpu_options.allow_growth = True\n",
    "\n",
    "# config = tf.contrib.learn.RunConfig(session_config = configconfig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
